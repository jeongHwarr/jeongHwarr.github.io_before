---
title: "[Review] SENet(Squeeze Excitation Networks)"
categories: Review
date: 2021-06-04 17:38:56 +0900
tags:
  - SENet
  - Squeeze Excitation Network
  - classification
  - SENet리뷰
  - SENet설명
toc: true
toc_sticky: true
---

# 1. SENet

SeNet은 Squeeze와 Excitation 방법을 적용한 SE Block을 사용한 네트워크입니다. SE Block의 동작은 다음과 같이 설명할 수 있습니다. 

- Squeeze operation: 각 피쳐맵에 대한 전체 정보를 요약하는 operation
- Excitation operation: 각 피쳐맵의 중요도를 스케일하는 operation

→ 각 피쳐맵에 중요도 정보가 추가된 피쳐를 통해 모델을 학습합니다. 

# 2. Squeeze-and-Excitation Blocks

![/assets/images/2021-06-04-SENet/untitled.png](/assets/images/2021-06-04-SENet/untitled.png)

$\mathbf{F}_{tr}$: 일반적인 컨벌루션 연산

$\mathbf{F}_{sq}$: squeeze 연산, 논문에선 GAP(Global Average Pooling)이 사용됨. 그 결과 1X1XC의 사이즈를 가지는 $z$가 만들어집니다. 

$\mathbf{F}_{ex}$: excitation 연산, sqeeze연산으로 인해 요약된 feature의 정보를 통해 구한 피쳐의 중요도 값 $s$를 구합니다. 

중요도 값 $s$는 아래의 식으로 구할 수 있습니다. 

$$s=\text{Sigmoid}(W_2\text{ReLU}(W_1z))$$

![/assets/images/2021-06-04-SENet/d6887251-026a-43c3-9791-3522b9878946.png](/assets/images/2021-06-04-SENet/d6887251-026a-43c3-9791-3522b9878946.png)

C는 채널 개수. 위 그림에서 C는 8, r은 4가 됩니다. 각 채널에 대한 GAP(Global Average Pooling)를 한 결과(그 결과 값은 채널 수 만큼 나옵니다)와 $w_1$ 곱한 값에 ReLU를 취해줍니다.그 값에 $w_2$를 곱해서 sigmoid를 취한 값이 중요도를 의미하게 됩니다.  


$\mathbf{F}_{scale}$: $\mathbf{F}_{ex}$의 결과 s를 이용해 원래 피쳐의 사이즈로 스케일합니다.. GAP 이전의 피쳐맵 $u_c$에 중요도 값인 $s_c$를 곱합니다.

$$\tilde{x}=\mathbf{F}_{scale}(u_c, s_c)=s_c\cdot{u_c}$$

즉, SE block은 채널축에 대해 feature map의 중요도를 구하고 그 중요도를 feature map에 반영하는 block이라고 정리할 수 있습니다. 

# 3. Reference

- [https://jayhey.github.io/deep learning/2018/07/18/SENet/](https://jayhey.github.io/deep%20learning/2018/07/18/SENet/)
